I"œ<ul id="markdown-toc">
  <li><a href="#the-intutive-definition-of-mutual-information" id="markdown-toc-the-intutive-definition-of-mutual-information">The intutive definition of Mutual Information</a></li>
  <li><a href="#lets-discuss-some-mathematical-discription-and-few-other-terms" id="markdown-toc-lets-discuss-some-mathematical-discription-and-few-other-terms">Letâ€™s discuss some mathematical discription and few other terms</a></li>
  <li><a href="#mutual-information-and-kl-divergence" id="markdown-toc-mutual-information-and-kl-divergence">Mutual information and KL divergence</a></li>
  <li><a href="#why-is-mutual-information-kl-divergnece-important" id="markdown-toc-why-is-mutual-information-kl-divergnece-important">Why is Mutual information, KL Divergnece important</a></li>
</ul>

<h3 id="the-intutive-definition-of-mutual-information">The intutive definition of Mutual Information</h3>

<p>Mutual information is the measure of <strong>reduction in the uncertainity</strong> about one quantity (a random variable), that is achieved <em>by observing another quantity</em> (another random variable).
<strong><em>Take for example :</em></strong> A car moving with certain velocity(say \(v_s\)) which is given by its on board sensor(A noisy speedometer), and there is a GPS receiver on the car which measures 
rate of change of position (velocity of the car) say \(v_g\), now since there is some process and measurment noise, we can say \(v_s\) and \(v_g\) are random variables, now the 
mutual information between these two, quantifies the reduction in uncertainity about \(v_s\), given \(v_g\).<br />
If the two random variables \(v_s\) and \(v_g\) are independent, the mutual information between them is zero. i.e. \(v_s\) and \(v_g\) donâ€™t share any information.</p>
<h3 id="lets-discuss-some-mathematical-discription-and-few-other-terms">Letâ€™s discuss some mathematical discription and few other terms</h3>
<p><strong>Conditional Entropy \(H(v_s|v_g)\):</strong><br />
It is the amount of information gained, once the outcome of \(v_s\) is known, given the outcome \(v_g\) <br />
<strong>Joint Entropy \(H(v_s,v_g)\) :</strong><br />
It is the entropy of joint distribution of two random variables.<br />
<br />
Let us denote Mutual information with \(I(v_s, v_g)\) ,<br />
\(I(v_s, v_g) \, = \sum \, p(v_s, v_g) \, \log_{2}\dfrac{p(v_s,v_g)}{p(v_s)p(v_g)}\)<br />
Let, \(H(.)\) denote the Shanonâ€™s Entropy function <br />
\(\begin{align}
I(v_s, v_g) \, &amp;= \, H(v_s)\, - \, H(v_s|v_g) \\
 &amp;= H(v_g)\, - \, H(v_g|v_s) \\
 &amp;= H(v_s)\, + \,H(v_g) \, - \, H(v_s,v_g)
\end{align}\)<br /></p>

<h3 id="mutual-information-and-kl-divergence">Mutual information and KL divergence</h3>
<p>KL divergence between any two random variable <strong>distributions</strong> tells us about, how different are the two distributions.
Mutual information tells us how much information one random variable contains, about another random variable.
<strong><em>one thing to always remember:</em></strong> lower the uncertainity, lower the information (Recall Claude Shannonâ€™s, definition of Information).</p>
<blockquote>
  <p>Now, think for few minutes about relating these two (Hint: Use the mathematical description of mutual information and Kl divergence)
<br /></p>
</blockquote>

<p><strong>The KL divergence \((D_{KL}(p||q))\):</strong><br />
The KL divergence between two probability distributions P and Q is,<br />
\(D_{KL}(P||Q)\, = \, \sum P\,\log_{2}\, \dfrac{P}{Q}\) <br />
<br /></p>

<p>Therefore, the Mutual information is the KL divergence between joint distribution and product distribution(The joint distribution if the two random variables were independent) of \(v_s\)  and \(v_g\)
i.e. \(I(v_s, v_g) \, = \, D_{KL}(p(v_g,v_s)\,||\, p(v_g)p(v_s))\), this is obvious by comparing the definitions of  Mutual information and KL Divergence.<br /></p>

<blockquote>
  <p>KL divergence is also know as <strong>relative entropy</strong><br />
But, why?
<br /></p>
</blockquote>

<p>Before proceeding with the answer, we must go through one more term i.e. The <strong>cross entropy</strong>.<br />
<strong>Cross Entropy \(H(P,Q)\):</strong><br />
When the true distribution is \(P\) (think of \(P\) as ground truth data distribution), the encoding of random variable can be based on another distribution \(Q\) (think of \(Q\) as the modelâ€™s output distribution) as a model that approximates \(P\). 
Then the average of the total number of bits needed to represent it, is called the cross-entropy.<br />
\(H(P,Q)\,=\,\sum_i\,P_i\,\log_2\,\dfrac{1}{Q_i}\)<br /></p>

<p><strong>Cross Entropy = KL divergence \(+\) Entropy</strong> (i.e \(H(P,Q) \, = \, D_{KL}(P||Q) \, + \, H(P)\)).<br />
<br /></p>

<blockquote>
  <p>Therefore, KL Divergence is the difference between Cross entropy and entropy<br />
Hence, KL divergence is also called as Relative entropy<br />
<br /></p>
</blockquote>

<h3 id="why-is-mutual-information-kl-divergnece-important">Why is Mutual information, KL Divergnece important</h3>
<p>1.While extracting features from a signal or image for classification using filters, it is best to choose the filterâ€™s characteristic function or 
weights such that KL divergence between the distribution of extracted features and class conditional distribution of extracted features is maximum.<br />
This is equivalent of maximizing mutual information between the signal/Data and classes. <br />
The Principle component analysis, used often in many signal processing and machine learning tasks, to select dimensions that capture maximum 
variance in the data, in fact maximizes this mutual information.<br />
<br /> <img src="/assets/linear_filtering.PNG" title="linearfiltering" width="90%" /> <br /></p>

<blockquote>
  <p>Mutual Information helps in selecting <strong>maximally informative dimensions</strong>
<br /></p>
</blockquote>

<p>2.Source coding, Channel coding in communication systems, mostly revolve around Mutual information.</p>

<blockquote>
  <p><strong>NOTE:</strong><br /> 
Mutual Information, joint Enropy and conditional entropy are referred in terms of random variables<br />
Cross Entropy, KL divergence are refered in terms of Probability distributions<br />
but, both are defined using probability distributions
<br /></p>
</blockquote>

:ET