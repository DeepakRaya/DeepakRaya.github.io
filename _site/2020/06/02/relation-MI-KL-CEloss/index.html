<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Relations between Mutual information, Cross entropy, KL Divergence</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="By Deepak Raya">
    <link rel="canonical" href="http://localhost:4000/2020/06/02/relation-MI-KL-CEloss/">
    <link href="/feed.xml" type="application/atom+xml" rel="alternate" title=" posts" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">
    <link rel="stylesheet" href="/css/style.css">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-122144402-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-122144402-1');
    </script>

</head>


    <body>

    <header class="site-header">

  <div class="wrap">

    <div style="float:left; margin-top:10px; margin-right:10px;">
   
    </div>

  <!-- <a class="site-title" href="#"></a> -->
  <a class="site-title" href="/"></a>
    
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">
        
          <a class="page-link" href="/projects/">Projects</a>
        
          <a class="page-link" href="/research/">Research</a>
        
          <a class="page-link" href="/learning-resources/">TA</a>
        
          <a class="page-link" href="/signal/">Blog</a>
        
          <a class="page-link" href="/talks/">Talks</a>
        
        <a class="page-link" href="https://deepakraya.github.io">About me</a>
      </div>
    </nav>
  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Relations between Mutual information, Cross entropy, KL Divergence</h1>
    <p class="meta">Jun 2, 2020</p>
  </header>

  <article class="post-content">
  <ul id="markdown-toc">
  <li><a href="#the-intutive-definition-of-mutual-information" id="markdown-toc-the-intutive-definition-of-mutual-information">The intutive definition of Mutual Information</a></li>
  <li><a href="#lets-discuss-some-mathematical-discription-and-few-other-terms" id="markdown-toc-lets-discuss-some-mathematical-discription-and-few-other-terms">Let’s discuss some mathematical discription and few other terms</a></li>
  <li><a href="#mutual-information-and-kl-divergence" id="markdown-toc-mutual-information-and-kl-divergence">Mutual information and KL divergence</a></li>
  <li><a href="#why-is-mutual-information-kl-divergnece-important" id="markdown-toc-why-is-mutual-information-kl-divergnece-important">Why is Mutual information, KL Divergnece important</a></li>
</ul>

<h3 id="the-intutive-definition-of-mutual-information">The intutive definition of Mutual Information</h3>

<p>Mutual information is the measure of <strong>reduction in the uncertainity</strong> about one quantity (a random variable), that is achieved <em>by observing another quantity</em> (another random variable).
<strong><em>Take for example :</em></strong> A car moving with certain velocity(say \(v_s\)) which is given by its on board sensor(A noisy speedometer), and there is a GPS receiver on the car which measures 
rate of change of position (velocity of the car) say \(v_g\), now since there is some process and measurment noise, we can say \(v_s\) and \(v_g\) are random variables, now the 
mutual information between these two, quantifies the reduction in uncertainity about \(v_s\), given \(v_g\).<br />
If the two random variables \(v_s\) and \(v_g\) are independent, the mutual information between them is zero. i.e. \(v_s\) and \(v_g\) don’t share any information.</p>
<h3 id="lets-discuss-some-mathematical-discription-and-few-other-terms">Let’s discuss some mathematical discription and few other terms</h3>
<p><strong>Conditional Entropy \(H(v_s|v_g)\):</strong><br />
It is the amount of information gained, once the outcome of \(v_s\) is known, given the outcome \(v_g\) <br />
<strong>Joint Entropy \(H(v_s,v_g)\) :</strong><br />
It is the entropy of joint distribution of two random variables.<br />
<br />
Let us denote Mutual information with \(I(v_s, v_g)\) ,<br />
\(I(v_s, v_g) \, = \sum \, p(v_s, v_g) \, \log_{2}\dfrac{p(v_s,v_g)}{p(v_s)p(v_g)}\)<br />
Let, \(H(.)\) denote the Shanon’s Entropy function <br />
\(\begin{align}
I(v_s, v_g) \, &amp;= \, H(v_s)\, - \, H(v_s|v_g) \\
 &amp;= H(v_g)\, - \, H(v_g|v_s) \\
 &amp;= H(v_s)\, + \,H(v_g) \, - \, H(v_s,v_g)
\end{align}\)<br /></p>

<h3 id="mutual-information-and-kl-divergence">Mutual information and KL divergence</h3>
<p>KL divergence between any two random variable <strong>distributions</strong> tells us about, how different are the two distributions.
Mutual information tells us how much information one random variable contains, about another random variable.
<strong><em>one thing to always remember:</em></strong> lower the uncertainity, lower the information (Recall Claude Shannon’s, definition of Information).</p>
<blockquote>
  <p>Now, think for few minutes about relating these two (Hint: Use the mathematical description of mutual information and Kl divergence)
<br /></p>
</blockquote>

<p><strong>The KL divergence \((D_{KL}(p||q))\):</strong><br />
The KL divergence between two probability distributions P and Q is,<br />
\(D_{KL}(P||Q)\, = \, \sum P\,\log_{2}\, \dfrac{P}{Q}\) <br />
<br /></p>

<p>Therefore, the Mutual information is the KL divergence between joint distribution and product distribution(The joint distribution if the two random variables were independent) of \(v_s\)  and \(v_g\)
i.e. \(I(v_s, v_g) \, = \, D_{KL}(p(v_g,v_s)\,||\, p(v_g)p(v_s))\), this is obvious by comparing the definitions of  Mutual information and KL Divergence.<br /></p>

<blockquote>
  <p>KL divergence is also know as <strong>relative entropy</strong><br />
But, why?
<br /></p>
</blockquote>

<p>Before proceeding with the answer, we must go through one more term i.e. The <strong>cross entropy</strong>.<br />
<strong>Cross Entropy \(H(P,Q)\):</strong><br />
When the true distribution is \(P\) (think of \(P\) as ground truth data distribution), the encoding of random variable can be based on another distribution \(Q\) (think of \(Q\) as the model’s output distribution) as a model that approximates \(P\). 
Then the average of the total number of bits needed to represent it, is called the cross-entropy.<br />
\(H(P,Q)\,=\,\sum_i\,P_i\,\log_2\,\dfrac{1}{Q_i}\)<br /></p>

<p><strong>Cross Entropy = KL divergence \(+\) Entropy</strong> (i.e \(H(P,Q) \, = \, D_{KL}(P||Q) \, + \, H(P)\)).<br />
<br /></p>

<blockquote>
  <p>Therefore, KL Divergence is the difference between Cross entropy and entropy<br />
Hence, KL divergence is also called as Relative entropy<br />
<br /></p>
</blockquote>

<h3 id="why-is-mutual-information-kl-divergnece-important">Why is Mutual information, KL Divergnece important</h3>
<p>1.While extracting features from a signal or image for classification using filters, it is best to choose the filter’s characteristic function or 
weights such that KL divergence between the distribution of extracted features and class conditional distribution of extracted features is maximum.<br />
This is equivalent of maximizing mutual information between the signal/Data and classes. <br />
The Principle component analysis, used often in many signal processing and machine learning tasks, to select dimensions that capture maximum 
variance in the data, in fact maximizes this mutual information.<br />
<br /> <img src="/assets/linear_filtering.PNG" title="linearfiltering" width="90%" /> &lt;figcaption&gt; Src: Larry F Abbot, Theoretical Neuroscience &lt;/figcaption&gt; <br /></p>

<blockquote>
  <p>Mutual Information helps in selecting <strong>maximally informative dimensions</strong>
<br /></p>
</blockquote>

<p>2.Source coding, Channel coding in communication systems, mostly revolve around Mutual information.</p>

<blockquote>
  <p><strong>NOTE:</strong><br /> 
Mutual Information, joint Enropy and conditional entropy are referred in terms of random variables<br />
Cross Entropy, KL divergence are refered in terms of Probability distributions<br />
but, both are defined using probability distributions
<br /></p>
</blockquote>


  </article>

  <!-- mathjax -->
  
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
  


      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <!-- <h2 class="footer-heading"></h2> -->

    <div class="footer-col-1 column">
      <ul>
        <li></li>
        <!-- <li><a href="mailto:v.deepakraya@gmail.com">v.deepakraya@gmail.com</a></li> -->
      </ul>
    </div>

    <div class="footer-col-2 column">
      <ul>
        <li>
          <a href="https://github.com/DeepakRaya">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">DeepakRaya</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/VDeepakRaya">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">VDeepakRaya</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text">By Deepak Raya</p>
    </div>

  </div>

</footer>

    </body>
</html>